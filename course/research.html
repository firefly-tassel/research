<!DOCTYPE HTML>
<!--
  Twenty by HTML5 UP
  html5up.net | @ajlkn
  Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Research</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-giJF6kkoqNQ00vy+HMDP7azOuL0xtbfIcaT9wjKHr8RbDVddVHyTfAAsrekwKmP1" crossorigin="anonymous">
    <link rel="shortcut icon" href="favicon.ico"/>
  <link rel="bookmark" href="favicon.ico"/>
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
  </head>
  <body class="no-sidebar is-preload">
    <div id="page-wrapper">

      <!-- Header -->
        <header id="header">
          <h1 id="logo" style="font-size: medium; font-weight: 500"><a href="index.html">Hung-yi Lee (李宏毅)</a></h1>
          <nav id="nav">
            <ul>
              <li class="menu"><a href="index.html">Home</a></li>
              <li class="menu"><a href="honor.html">Honor</a></li>
              <li class="current"><a href="research.html">Research</a></li>
              <li class="menu"><a href="talk.html">Talk</a></li>
              <li class="menu"><a href="publication.html">Publication</a></li>
              <li class="submenu">
                <a href="#">Course</a>
                <ul>
                  <li class="submenu">
                    <a href="#">Machine Learning</a>
                    <ul>
                      <li><a href="ml/2021-spring.html">2021 Spring</a></li>
                      <li><a href="ml/2020-spring.html">2020 Spring</a></li>
                      <li><a href="ml/2019-spring.html">2019 Spring</a></li>
                      <li><a href="ml/2017-fall.html">2017 Fall</a></li>
                      <li><a href="ml/2017-spring.html">2017 Spring</a></li>
                      <li><a href="ml/2016-fall.html">2016 Fall</a></li>
                    </ul>
                  </li>
                  <li class="submenu">
                    <a href=#>DLHLP</a>
                    <ul>
                      <li><a href="dlhlp/2020-spring.html">2020 Spring</a></li>
                    </ul>
                  </li>
                  <li class="submenu">
                    <a href="#">MLDS</a>
                    <ul>
                      <li><a href="mlds/2018-spring.html">2018 Spring</a></li>
                      <li><a href="mlds/2017-spring.html">2017 Spring</a></li>
                      <li><a href="mlds/2015-fall.html">2015 Fall</a></li>
                    </ul>
                  </li>
                  <li class="submenu">
                    <a href="#">Linear Algebra</a>
                    <ul>
                      <li><a href="la/2021-fall.html">2021 Fall</a></li>
                      <li><a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/LA_2020/policy.pdf" target="_blank" rel="noreferrer noopener">2020 Fall</a></li>
                      <li><a href="la/2019-fall.html">2019 Fall</a></li>
                      <li><a href="la/2018-fall.html">2018 Fall</a></li>
                      <li><a href="la/2016-spring.html">2016 Spring</a></li>
                    </ul>
                  </li>
                  <li><a href="circuit/2014-fall.html">Circuit</a></li>
                </ul>
              </li>
              <li class="menu"><a href="https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ/playlists" target="_blank" rel="noopener noreferrer">Youtube</a></li>
            </ul>
          </nav>

        </header>

      <!-- Main -->
        <article id="main">

          <header class="special container">
            <span class="icon solid fas fa-glasses"></span>
            <h2>Research</h2>
          </header>
          <section>
		    <div class="container">
			  <div class="row justify-content-md-center" style="font-size: large; background-color: #f0f0f5; border-radius: 50px">
			    <div class="col" style="text-align: justify; margin-left: 5px; margin-right: 5px">
				  <ul>
				    

					<li><b>My research team uses deep learning to develop a series of language understanding and speech processing technology. We proposed a series of technology remarkably reducing the requirement of annotated data for language understanding and speech processing.</b></li>

					<ul>

					<li>Sequence Generation by Generative Adversarial Network (GAN): Proposing novel training algorithm of GAN for sequential data<a href="https://ieeexplore.ieee.org/document/8403313"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>. 
					Compared with the previous state-of-the-art approach, the training speed of the proposed approach is five times faster. 
					Because text and speech are intrinsically sequential data, the proposed approach is very useful for language understanding and speech processing.</li>

					<li>Unsupervised Speech Recognition</li>	
					<ul>

					<li>Audio word2vec: Unsupervised learning of audio segment representations using sequence-to-sequence autoencoder
					<a href="https://arxiv.org/pdf/1603.00982.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.</li>
					<li>Audio segment representations learned from one language can be applied to other languages
					<a href="https://arxiv.org/pdf/1707.06519.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.</li>
					<li>Audio segment representations improve video captioning
					<a href="paper/CaptionASRU17.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.</li></li>
					<li>Without text transcription, machine automatically learns to identify word boundaries from audio
					<a href="https://ieeexplore.ieee.org/document/8736337"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.</li></li>

					<li>Developing the world’s first unsupervised speech recognition system 
					<a href="https://arxiv.org/pdf/1804.00316.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>. 
					Without paired text, it has achieved a phoneme recognition error rate of 33% 
					<a href="https://arxiv.org/pdf/1904.04100.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>. 
					</li>

					<!--<li>we analyze the gate activation signals inside the gated recurrent neural networks, 
					and find the temporal structure of such signals is highly correlated with the phoneme boundaries. 
					(<a href="https://arxiv.org/pdf/1703.07588.pdf">ref</a>).</li>-->
					</ul>



					<li>Unsupervised Voice conversion (VC)</li>
					<ul>
					<li>
					Transforming the voice of speaker A into speaker B can be considered as a typical example of VC. 
					To achieve that, usually speakers A and B have to read hundreds of sentences with the same content to teach machine how to transform their voices, which is not practical. 
					Some new VC approaches are proposed based on GAN 
					<a href="https://arxiv.org/pdf/1904.05742.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					<a href="https://arxiv.org/pdf/1804.02812.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					(one of the 12 finalists for the best student paper award of INTERSPEECH 2018). 
					Only the audio content of speakers A and B are needed, and they do not have to read the same sentences. 
					</li>
					</ul>

					<li>Reading/Listening Comprehension</li>
					<ul>

					<li>Developing the world’s first spoken language understanding system that can take TOEFL Listening Comprehension Test
					<a href="https://arxiv.org/pdf/1608.06378.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					(the paper nominated for the Best Student Paper Award in INTERSPEECH 2016).
					Machine achieved 49% accuracy 
					<a href="https://arxiv.org/pdf/1608.07775.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.
					Machine achieved 55% accuracy 
					<a href="https://arxiv.org/pdf/1711.05345.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.
					<a href="https://github.com/iamyuanchung/TOEFL-QA">dataset</a>
					</li>

					<li>Developing the world’s first deep learning based spoken QA system (QA system that can answer questions based on spoken content) and benchmark corpora
					<a href="https://arxiv.org/pdf/1804.00320.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					<a href="https://arxiv.org/pdf/1808.02280.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					.
					</li>
					<li>Using GAN to deal with the problem of the lack of training data for spoken QA<a href="https://arxiv.org/pdf/1904.07904.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.</li>
					<li>Proposing to learn Chinese word representations from glyphs of characters<a href="https://arxiv.org/pdf/1708.04755.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>.</li>
					<li>Leading a team participating Formosa Grand Challenge competition held by the Ministry of Science and Technology of Taiwan. 
					The competition is listening comprehension of Chinese spoken content by machine. 143 teams participated in the competition.
					We won the champion in the final competition(<a href="https://udn.com/news/story/7270/3714842">news</a>).</li>
					<li>Leading a team participating MovieQA competition 2017, in which machine answers questions based on the plots of movies. The team ranked at the 2nd place
					(<a href="http://movieqa.cs.toronto.edu/leaderboard/#table-plot">leaderboard</a>).</li> 
					</ul>

					<li>Information Extraction from Text/Spoken Content</li>
					<ul>

					<li>
					Unsupervised Abstractive Summarization: Abstractive summarization is to generate a summary that describes the core ideas of the document in its own words. 
					To train a summarizer with reasonable performance, in general, millions of paired documents and summaries as training examples are needed, which limits the application of the technology. 
					Based on GAN, we propose unsupervised abstractive summarization, and the approach achieves performance comparable with the state-of-the-art approaches but with only 20% of the paired data
					<a href="http://speech.ee.ntu.edu.tw/~tlkagk/paper/learning-encode-text.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>. 
					</li>
					<li>Abstractive summarization for spoken content using connectionist temporal classification (CTC)
					<a href="paper/CTCIS17.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					.</li>
					<li>Abstractive summarization for spoken content using attention-based sequence-to-sequence network with ASR error modeling 
					<a href="https://arxiv.org/pdf/1612.08375.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					.</li>
					<li>Key term extraction using neural attention models
					<a href="https://arxiv.org/pdf/1604.00077.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					.</li>
					</ul>
					<!--<ul> demo system ????? -->
					<!--<ul>
					<li>Proposing and developing the new approaches of directly selecting the best utterance subset to be the summary completely different from the conventional approaches considering utterances individually</li>
					</ul>-->

					<li>Human-machine Interaction</li>
					<ul>

					<li>Reinforcement Learning: Proposing to apply reinforcement learning in human-machine interaction to determine the machine actions for interactive retrieval 
					<a href="https://arxiv.org/pdf/1609.05234.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					<a href="paper/WenIS12.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a> 
					(the paper nominated for the Best Student Paper Award in INTERSPEECH 2012). 

					<li>Learnable Simulated User: However, reinforcement learning relies on hand-crafted user simulators. 
					Building a reliable user simulator is difficult and expensive. 
					Inspired from the framework in GAN, we proposed to further improve human-machine interaction by proposing a learnable user simulator which is jointly trained with an interactive agent, precluding the need for a hand-crafted user simulator 
					<a href="https://arxiv.org/pdf/1804.00318.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>. 
					This paper won the best student paper award of INTERSPEECH 2018 (3 out of 700).

					<li>Style Controllable Chatbot: 
					The conventional chatbot is in general emotionless, and this is a major limitation because the emotion plays a critical role in human social interactions. 
					We propose to train the chatbot to generate responses with scalable sentiment by setting the mode for chatting
					<a href="https://arxiv.org/pdf/1804.02504.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>. 
					This can be achieved by GAN which transforms the style of chatbot response.
					The techniques mentioned here is extended to conversational style adjustment, so the machine may imitate the conversational style of someone the user is familiar with, to make the chatbot more friendly or more personal
					<a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1696.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>. 
					Demo at the 2016 Intel Asia Innovation Summit 
					(<a href="https://www.bnext.com.tw/article/41871/terrygou-trump-chatbot">news</a>).
					</li>

					</ul>

					<li>Speech recognition based on deep learning technology</li>	
					<ul>
					<li>Personalized speech recognizer</li>
					<ul>
					<li>Proposing using the posts on social network to 
					personalize recurrent neural network based language models  
					(a paper nominated for the Best Student Paper Award in INTERSPEECH 2013<a href="paper/WenIS13.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>)
					and Word Embedding <a href="paper/WEASRU17.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a></li>
					<!--<li>Video Demonstration</li><iframe width="420" height="315" src="//www.youtube.com/embed/2fBbaBcq6mw" frameborder="0" allowfullscreen></iframe>-->
					</ul>
					<li>Language modeling with Neural Turing Machine<a href="paper/NTMICASSP17.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a></li>
					<li>Acoustic modeling with Structured Deep Network<a href="paper/DNN_ASRU15.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a></li>
					</ul>



					<li>Spoken Content Retrieval</li>
					<ul>
					<li>
					I'm a co-author of a <b>tutorial paper</b> summarizing the recent study
					<a href="paper/Overview.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					</li>
					<li>Proposing the innovative directions beyond the mainstream approaches of cascading speech recognition and text information retrieval with performance shown to be significantly less constrained by recognition errors:</li>
					<ul>
					<li>Relevance Feedback: Proposing the new framework integrating recognition and retrieval by user relevance feedback (mentioned in <a href=http://books.google.com.tw/books/about/Spoken_Language_Understanding.html?id=RDLyT2FythgC&redir_esc=y>textbook</a>), and a series of approaches using acoustic feature space similarity 
					(a paper nominated for the Best Student Paper Award in ASRU 2011<a href="paper/TuASRU11.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>)</li>
					<li>Semantic Retrieval: Proposing new frameworks by acoustic feature similarity, context consistency, and query expansion based on automatically discovered acoustic patterns 
					(a paper received Spoken Language Processing Student Travel Grant in ICASSP 2012<a href="paper/TuICASSP12.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>)
					<li>Unsupervised Semantic Retrieval: Proposing the novel approach for semantic retrieval of spoken content without using speech recognition at all
					<a href="http://ieeexplore.ieee.org/abstract/document/6707729/"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a></li>
					<li>Parameter Learning: Proposing the novel approach of learning the weights on the indexing features by optimizing the evaluation metrics (mentioned in <a href=http://books.google.com.tw/books/about/Spoken_Language_Understanding.html?id=RDLyT2FythgC&redir_esc=y>textbook</a>)</li>
					</ul>
					</ul>

					<!--<ul><li>Leading a team participating in the Computational Paralinguistics Challenge (ComParE) in Interspeech 2013</li></ul>-->

					</ul>

					<li><b>The developed techniques have the following applications:</b></li>
					<ul>

					<li>Participating in research on managing and organizing knowledge from on-line course materials in Spoken Language Systems Group, Computer Science and Artificial Intelligence Lab (CSAIL), Massachusetts Institute of Technology (MIT)</li>
					<ul>
					<li>A platform developed for helping learners take on-line courses by analyzing,  discovering and visualizing the relationships among lectures from similar courses and textbooks of related subjects
					<a href="paper/MOOCIS15.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>
					<!--<li>Giving a talk at the Quanta Computer, Taoyuan, Taiwan (January 2014)</li> -->
					<li>Video Demonstration</li>
					<iframe width="560" height="315" src="//www.youtube.com/embed/VGKvPShhmiQ" frameborder="0" allowfullscreen></iframe>
					</ul>

					<li>Participating in the IARPA Babel Program in Spoken Language Systems Group, CSAIL, MIT</li>
					<ul>
					<li>Using acoustic feature space similarity to significantly improve the performance of spoken content retrieval for three different languages (Assamese, Bengali and Lao)
					<a href="paper/BabelIS14.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a></li>
					<!--<li>Giving a talk in the IARPA Babel PI meeting, Baltimore (January 2014)</li>-->
					</ul>

					<li>Participating in developing the prototype system of NTU Virtual Instructor in Digital Speech Processing Laboratory, NTU</li>
					<ul>
					<li>An on-line learning platform organizing spoken knowledge in course lectures for efficient personalized learning 
					(responsible for the spoken content retrieval part) 
					<a href="paper/LectureFinal.pdf"class="btn btn-outline-primary  p-1" role="button" aria-pressed="true">pdf</a>, <a href="slide/MyTalk_NTUCS_v8.pdf">slides</a></li>
					<li><a href="http://140.112.21.15/~RA/lecture/">Demo System</a> (please browse it by FireFox or Chrome)</li>
					<!--<li>Giving a talk at the Department of Computer Science & Information Engineering of National Taiwan University (March 2014) (<a href="slide/MyTalk_NTUCS_v8.pdf">slides</a>)</li>-->
					</ul>
						
						</ul>	

						</ul>

			      </ul>
				</div>
			  </div>
			</div>
		  </section>
        </article>

      <!-- Footer -->
        <footer id="footer">

          <ul class="icons">
            <li><a href="https://www.facebook.com/profile.php?id=100000149111577" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
            <li><a href="https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ/playlists" class="icon brands fa-youtube"><span class="label">Youtube</span></a></li>
          </ul>

          <ul class="copyright">
            <li>&copy; Untitled</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
          </ul>

        </footer>

    </div>

    <!-- Scripts --> 
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
      <script src="assets/js/jquery.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta1/dist/js/bootstrap.bundle.min.js" integrity="sha384-ygbV9kiqUc6oa4msXn9868pTtWMgiQaeYH7/t7LECLbyPA2x65Kgf80OJFdroafW" crossorigin="anonymous"></script>
      <script src="assets/js/jquery.dropotron.min.js"></script>
      <script src="assets/js/jquery.scrolly.min.js"></script>
      <script src="assets/js/jquery.scrollgress.min.js"></script>
      <script src="assets/js/jquery.scrollex.min.js"></script>
      <script src="assets/js/browser.min.js"></script>
      <script src="assets/js/breakpoints.min.js"></script>
      <script src="assets/js/util.js"></script>
      <script src="assets/js/main.js"></script> 
  </body>
</html>
